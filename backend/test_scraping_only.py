#!/usr/bin/env python3
"""
ìŠ¤í¬ë˜í•‘ ê°€ëŠ¥ ì—¬ë¶€ë§Œ í…ŒìŠ¤íŠ¸ (AI ë¶„ì„ ì—†ì´)
í† í° ì†Œëª¨ ì—†ì´ ê¸°ì‚¬ ì¶”ì¶œ ê°€ëŠ¥ ì—¬ë¶€ë§Œ í™•ì¸í•©ë‹ˆë‹¤.
"""

import sys
sys.path.insert(0, '/Users/gamnamu/Documents/cr-check/backend')

from scraper import ArticleScraper
import json
from datetime import datetime

def test_url(scraper, url, name=""):
    """ë‹¨ì¼ URL ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸"""
    print(f"\n{'='*70}")
    print(f"ğŸ“° í…ŒìŠ¤íŠ¸: {name if name else url}")
    print(f"{'='*70}")
    
    try:
        result = scraper.scrape(url)
        
        # ê²°ê³¼ ê²€ì¦
        title = result.get("title", "")
        content = result.get("content", "")
        publisher = result.get("publisher", "ë¯¸í™•ì¸")
        journalist = result.get("journalist", "ë¯¸í™•ì¸")
        publish_date = result.get("publish_date", "ë¯¸í™•ì¸")
        
        # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
        is_valid = len(title) > 5 and len(content) > 100
        
        if is_valid:
            print(f"âœ… ìŠ¤í¬ë˜í•‘ ì„±ê³µ")
            print(f"   ì œëª©: {title[:80]}{'...' if len(title) > 80 else ''}")
            print(f"   ë³¸ë¬¸ ê¸¸ì´: {len(content):,}ì")
            print(f"   ë§¤ì²´ëª…: {publisher}")
            print(f"   ê¸°ìëª…: {journalist}")
            print(f"   ê²Œì¬ì¼: {publish_date}")
            
            return {
                "status": "success",
                "url": url,
                "name": name,
                "title": title,
                "content_length": len(content),
                "publisher": publisher,
                "journalist": journalist,
                "publish_date": publish_date,
                "content_preview": content[:200]
            }
        else:
            print(f"âš ï¸  ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ (ì œëª© ë˜ëŠ” ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ)")
            print(f"   ì œëª© ê¸¸ì´: {len(title)}ì")
            print(f"   ë³¸ë¬¸ ê¸¸ì´: {len(content)}ì")
            
            return {
                "status": "failed",
                "url": url,
                "name": name,
                "error": "ì œëª© ë˜ëŠ” ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ",
                "title_length": len(title),
                "content_length": len(content)
            }
            
    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨")
        print(f"   ì—ëŸ¬: {str(e)}")
        
        return {
            "status": "error",
            "url": url,
            "name": name,
            "error": str(e)
        }

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    scraper = ArticleScraper()
    
    # í…ŒìŠ¤íŠ¸í•  URL ë¦¬ìŠ¤íŠ¸
    # í˜•ì‹: (ì–¸ë¡ ì‚¬ëª…, URL)
    test_cases = [
        # ì¤‘ì•™ì¼ê°„ì§€ 12ê³³
        ("ê²½í–¥ì‹ ë¬¸", "https://www.khan.co.kr/article/202512031831001"),
        ("êµ­ë¯¼ì¼ë³´", "https://www.kmib.co.kr/article/view.asp?arcid=0029062500&code=61111611&sid1=pol"),
        ("ë‚´ì¼ì‹ ë¬¸", "https://www.naeil.com/news/read/569720"),
        ("ë™ì•„ì¼ë³´", "https://www.donga.com/news/Economy/article/all/20251203/132892988/1"),
        ("ë¬¸í™”ì¼ë³´", "https://www.munhwa.com/article/11551636"),
        ("ì„œìš¸ì‹ ë¬¸", "https://www.seoul.co.kr/news/economy/distribution/2025/12/04/20251204008005"),
        ("ì„¸ê³„ì¼ë³´", "https://www.segye.com/newsView/20251202516380"),
        ("ì•„ì‹œì•„íˆ¬ë°ì´", "https://www.asiatoday.co.kr/kn/view.php?key=20251203010001955&ref=main_midtop&ref=section_topnews"),
        ("ì¡°ì„ ì¼ë³´", "https://www.chosun.com/economy/industry-company/2025/12/03/3H7ED2VJFZH4DPFQVODQP6QDOE/"),
        ("ì¤‘ì•™ì¼ë³´", "https://www.joongang.co.kr/article/25387257"),
        ("í•œê²¨ë ˆ", "https://www.hani.co.kr/arti/society/society_general/1232666.html"),
        ("í•œêµ­ì¼ë³´", "https://www.hankookilbo.com/News/Read/A2025120113540002855"),
    ]
    
    print("\n" + "="*70)
    print("ğŸ” ìŠ¤í¬ë˜í•‘ ê°€ëŠ¥ ì—¬ë¶€ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"   ì´ {len(test_cases)}ê°œ URL í…ŒìŠ¤íŠ¸")
    print("="*70)
    
    results = []
    success_count = 0
    
    for name, url in test_cases:
        result = test_url(scraper, url, name)
        results.append(result)
        
        if result["status"] == "success":
            success_count += 1
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print("\n" + "="*70)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("="*70)
    print(f"âœ… ì„±ê³µ: {success_count}/{len(test_cases)}")
    print(f"âŒ ì‹¤íŒ¨: {len(test_cases) - success_count}/{len(test_cases)}")
    
    # ì„±ê³µ/ì‹¤íŒ¨ ëª©ë¡
    print("\n[ì„±ê³µí•œ ì–¸ë¡ ì‚¬]")
    for r in results:
        if r["status"] == "success":
            print(f"  âœ… {r['name']}")
    
    print("\n[ì‹¤íŒ¨í•œ ì–¸ë¡ ì‚¬]")
    for r in results:
        if r["status"] != "success":
            print(f"  âŒ {r['name']}: {r.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    output_file = f'/Users/gamnamu/Documents/cr-check/backend/scraping_test_result_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "test_date": datetime.now().isoformat(),
            "total_tests": len(test_cases),
            "success_count": success_count,
            "results": results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
    print("="*70 + "\n")

if __name__ == "__main__":
    main()
